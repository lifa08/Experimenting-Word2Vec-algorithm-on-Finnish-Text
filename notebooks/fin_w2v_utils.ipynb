{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from voikko.libvoikko import Voikko\n",
    "import os\n",
    "from collections import Counter\n",
    "import random\n",
    "import urllib\n",
    "import tarfile\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data file\n",
    "\n",
    "Data file can be downloaded from [here](http://statmt.org/wmt17/translation-task.html#download). \n",
    "Since we need only the Finnish text to train a word2vec model, we just need to download the monolingual language model training data.\n",
    "\n",
    "**Note**: the download link after click is: http://statmt.org/wmt15/europarl-v8.fi.tgz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_mkdir(path):\n",
    "    '''Create a directory if there isn't one already.'''\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unzip_and_remove(zipped_file, unzip_dir):\n",
    "    print('unzipping file...')\n",
    "    tar = tarfile.open(zipped_file, 'r')\n",
    "    tar.extractall(path=unzip_dir)\n",
    "    tar.close()\n",
    "    os.remove(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_file(download_url,\n",
    "                       data_dir,\n",
    "                       local_dest, \n",
    "                       expected_byte):\n",
    "    \"\"\" \n",
    "    Download the file from download_url into local_dest\n",
    "    if the file doesn't already exists.\n",
    "    Check if the downloaded file has the same number of bytes as expected_byte.\n",
    "    Unzip the file and remove the zip file\n",
    "    \"\"\"\n",
    "    unzip_name = local_dest[:-4]\n",
    "\n",
    "    if os.path.exists(unzip_name):\n",
    "        print('file already exists')\n",
    "        return unzip_name\n",
    "    elif os.path.exists(local_dest):\n",
    "        print('file already exists but unzipped')\n",
    "        unzip_and_remove(local_dest, data_dir)\n",
    "        return unzip_name\n",
    "    else:\n",
    "        safe_mkdir(data_dir)\n",
    "\n",
    "        print('Downloading...')\n",
    "        _, _ = urllib.request.urlretrieve(download_url, local_dest)\n",
    "        file_stat = os.stat(local_dest)\n",
    "\n",
    "        if file_stat.st_size == expected_byte:\n",
    "            print('Successfully downloaded')\n",
    "        else:\n",
    "            print('The downloaded file has unexpected number of bytes')\n",
    "            return\n",
    "\n",
    "        unzip_and_remove(local_dest, data_dir)\n",
    "        return unzip_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "Successfully downloaded\n",
      "unzipping file...\n"
     ]
    }
   ],
   "source": [
    "download_url = 'http://statmt.org/wmt15/europarl-v8.fi.tgz'\n",
    "\n",
    "script_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "project_dir = os.path.dirname(script_dir)\n",
    "data_dir = os.path.join(project_dir, 'data')\n",
    "data_file_name = 'europarl-v8.fi.tgz'\n",
    "data_file_path = os.path.join(data_dir, data_file_name)\n",
    "\n",
    "expected_byte = 99540237\n",
    "\n",
    "data_file_path = download_data_file(download_url, data_dir, data_file_path, expected_byte)\n",
    "assert data_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data file and tokenize into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    ''' Read data into a list of words and store the words into a file\n",
    "    if the relevant word file does not exist'''\n",
    "\n",
    "    if os.path.exists(file_path + '_words'):\n",
    "        print('reading from word file...')\n",
    "        with open(file_path + '_words', 'r') as f:\n",
    "            words = f.read().split('\\n')\n",
    "            return words\n",
    "\n",
    "    print('reading from data file...')\n",
    "    v = Voikko(\"fi\")\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        words = [word.tokenText.lower() for word in v.tokens(f.read())\n",
    "                 if word.tokenType==1 or word.tokenType==2]\n",
    "        # print(words)\n",
    "        v.terminate()\n",
    "\n",
    "        file = open(file_path + '_words', 'w')\n",
    "        file.write('\\n'.join(words))\n",
    "        file.close()\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from data file...\n",
      "['istuntokauden', 'uudelleenavaaminen', 'julistan', 'perjantaina', 'joulukuun', '17', '.', 'päivänä', 'keskeytetyn', 'euroopan', 'parlamentin', 'istunnon', 'avatuksi', 'ja', 'esitän', 'vielä', 'kerran', 'vilpittömän', 'toiveeni', 'siitä', ',', 'että', 'teillä', 'olisi', 'ollut', 'oikein', 'mukava', 'joululoma', '.', 'kuten', 'olette', 'varmaan', 'saattaneet', 'huomata', ',', 'vuodenvaihteeseen', '2000', 'povattuja', 'suuria', 'tietokoneongelmia', 'ei', 'ilmennytkään', '.', 'sen', 'sijaan', 'todella', 'kauheat', 'luonnonkatastrofit', 'koettelivat', 'kansalaisia', 'joissakin', 'unionimme', 'maissa', '.', 'te', 'olette', 'esittäneet', 'toiveen', ',', 'että', 'tästä', 'asiasta', 'keskusteltaisiin', 'lähipäivinä', 'tämän', 'istuntojakson', 'aikana', '.', 'sillä', 'välin', 'toivoisin', ',', 'kuten', 'useampi', 'kollega', 'on', 'minulle', 'esittänytkin', ',', 'että', 'viettäisimme', 'minuutin', 'hiljaisuuden', 'kaikkien', 'niiden', 'uhrien', 'muistoksi', ',', 'jotka', 'saivat', 'surmansa', 'useita', 'euroopan', 'unionin', 'maita', 'koetelleissa', 'myrskyissä', '.', 'kehotan', ',']\n"
     ]
    }
   ],
   "source": [
    "words = read_data(data_file_path)\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a vocabulary for the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(words, vocab_size, vocab_dir, vocab_file_path):\n",
    "    '''Build vocabulary of vocab_size most frequent words and write it to vocab_file_path.\n",
    "        words: a list of words.\n",
    "    '''\n",
    "    print(\"building vocabulary...\")\n",
    "\n",
    "    dictionary = dict()\n",
    "    index = 0\n",
    "\n",
    "    if words == None:\n",
    "        with open(vocab_file_path, 'r') as f:\n",
    "            words = f.read().split('\\n')\n",
    "            for word in words:\n",
    "                dictionary[word] = index\n",
    "                index += 1\n",
    "    else:\n",
    "        safe_mkdir(vocab_dir)\n",
    "        file = open(vocab_file_path, 'w')\n",
    "        count = [('UNK', -1)]\n",
    "        count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = index\n",
    "            index += 1\n",
    "            file.write(word + '\\n')\n",
    "        file.close()\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocabulary...\n",
      "0\n",
      "5\n",
      "287\n"
     ]
    }
   ],
   "source": [
    "vocab_dir = os.path.join(project_dir, 'vocab')\n",
    "vocab_file_name = 'fin_vocab.tsv'\n",
    "vocab_file_path = os.path.join(vocab_dir, vocab_file_name)\n",
    "vocab_size = 10000\n",
    "\n",
    "if os.path.exists(vocab_file_path):\n",
    "    dictionary =  build_vocab(None, 0, vocab_dir, vocab_file_path)\n",
    "else:\n",
    "    dictionary = build_vocab(words, vocab_size, vocab_dir, vocab_file_path)\n",
    "    del words\n",
    "\n",
    "print(dictionary['UNK']) # 0\n",
    "print(dictionary['että']) # 5\n",
    "print(dictionary['todellakin']) # 287"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert words to their correspond index in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_index(index_file, file_path, dictionary):\n",
    "    '''Read sentences from file and replace them with\n",
    "    their corresponding word indices in the dictionary'''\n",
    "\n",
    "    print(\"converting sentences to indices...\")\n",
    "    v = Voikko(\"fi\")\n",
    "\n",
    "    index_f = open(index_file, 'wb')\n",
    "    with open(file_path) as f:\n",
    "        index_sentences = []\n",
    "        for sentence in f:\n",
    "            words = [word.tokenText.lower() for word in v.tokens(sentence)\n",
    "                 if word.tokenType==1 or word.tokenType==2]\n",
    "\n",
    "            # print(words)\n",
    "            index_words = [dictionary[word] if word in dictionary else 0 for word in words]\n",
    "            index_sentences.append(index_words)\n",
    "        v.terminate()\n",
    "\n",
    "        # save sentence indices into a index_file\n",
    "        pkl.dump(index_sentences, index_f, -1)\n",
    "        index_f.close()\n",
    "        \n",
    "        return index_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting sentences to indices...\n"
     ]
    }
   ],
   "source": [
    "index_file = os.path.join(data_dir, 'sentence_idx_imdb.pkl')\n",
    "\n",
    "if os.path.exists(index_file):\n",
    "    with open(index_file, 'rb') as f:\n",
    "        sentence_indices = pkl.load(f)\n",
    "else:\n",
    "    sentence_indices = sentence_to_index(index_file, data_file_path, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4199, 0], [5654, 5933, 4556, 1501, 2, 952, 0, 6, 26, 1616, 0, 3, 1368, 67, 246, 0, 0, 22, 1, 5, 1663, 33, 105, 380, 0, 0, 2], [42, 401, 5038, 8482, 3661, 1, 0, 381, 0, 667, 0, 7, 0, 2], [12, 370, 180, 0, 0, 0, 670, 1091, 9084, 392, 2], [323, 401, 1795, 8856, 1, 5, 74, 199, 0, 0, 25, 6211, 145, 2], [56, 3637, 3427, 1, 42, 0, 1055, 4, 515, 0, 1, 5, 0, 2847, 0, 149, 64, 1862, 0, 1, 21, 3563, 8269, 434, 6, 30, 907, 0, 0, 2], [871, 1, 5, 0, 0, 25, 2847, 0, 5053], [18, 86, 0, 0, 2847, 0, 2, 19], [11, 20, 1, 2870, 6703, 2], [401, 5038, 687, 0, 6015, 3, 0, 417, 1, 5, 5720, 0, 4, 0, 434, 4962, 0, 0, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(sentence_indices[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4199, 0], [5654, 5933, 4556, 1501, 2, 952, 0, 6, 26, 1616, 0, 3, 1368, 67, 246, 0, 0, 22, 1, 5, 1663, 33, 105, 380, 0, 0, 2], [42, 401, 5038, 8482, 3661, 1, 0, 381, 0, 667, 0, 7, 0, 2], [12, 370, 180, 0, 0, 0, 670, 1091, 9084, 392, 2], [323, 401, 1795, 8856, 1, 5, 74, 199, 0, 0, 25, 6211, 145, 2], [56, 3637, 3427, 1, 42, 0, 1055, 4, 515, 0, 1, 5, 0, 2847, 0, 149, 64, 1862, 0, 1, 21, 3563, 8269, 434, 6, 30, 907, 0, 0, 2], [871, 1, 5, 0, 0, 25, 2847, 0, 5053], [18, 86, 0, 0, 2847, 0, 2, 19], [11, 20, 1, 2870, 6703, 2], [401, 5038, 687, 0, 6015, 3, 0, 417, 1, 5, 5720, 0, 4, 0, 434, 4962, 0, 0, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(sentence_indices[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a generator to construct training pairs according to the skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(index_words, context_window_size):\n",
    "    '''Form training pairs according to the skip-gram model.'''\n",
    "    for sentence_words in index_words:\n",
    "        for index, center in enumerate(sentence_words):\n",
    "            context = random.randint(1, context_window_size)\n",
    "            # print(context)\n",
    "\n",
    "            # get a random number of targets before the center word\n",
    "            for target in sentence_words[max(0, index - context): index]:\n",
    "                yield center, target\n",
    "\n",
    "            # get a random number of targets after the center word\n",
    "            for target in sentence_words[index + 1: index + context + 1]:\n",
    "                yield center, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4199, 0)\n"
     ]
    }
   ],
   "source": [
    "skip_window = 2\n",
    "single_gen = generate_sample(sentence_indices, skip_window)\n",
    "print(next(single_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 4199)\n"
     ]
    }
   ],
   "source": [
    "print(next(single_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 5933)\n"
     ]
    }
   ],
   "source": [
    "print(next(single_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lemmatize a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_file(filename):\n",
    "    print('lemmatizing ' + filename)\n",
    "\n",
    "    v = Voikko(\"fi\")\n",
    "    lemmatized_filename = filename + '_lemmatized'\n",
    "    lemmatized_file = open(lemmatized_filename, 'w') \n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for sentence in f:\n",
    "            sent_toks = v.tokens(sentence)\n",
    "\n",
    "            words_baseform = []\n",
    "            for word in sent_toks:\n",
    "                if word.tokenType == 1:\n",
    "                    word_analyzed = v.analyze(word.tokenText)\n",
    "                    if len(word_analyzed) > 0:\n",
    "                        words_baseform.append(word_analyzed[0].get('BASEFORM'))\n",
    "                    else:\n",
    "                        words_baseform.append(word.tokenText)\n",
    "                else:\n",
    "                    words_baseform.append(word.tokenText)\n",
    "\n",
    "            sent_baseform = ''.join(words_baseform)\n",
    "            lemmatized_file.write(sent_baseform)\n",
    "\n",
    "    lemmatized_file.close()\n",
    "    v.terminate()\n",
    "    return lemmatized_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all of above in one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gen(download_url, data_dir, data_file_path, expected_byte,\n",
    "              vocab_dir, vocal_file_path, vocab_size, index_file,\n",
    "              batch_size, skip_window, lemmatize):\n",
    "    if os.path.exists(index_file):\n",
    "        with open(index_file, 'rb') as f:\n",
    "            sentence_indices = pkl.load(f)\n",
    "\n",
    "    else:\n",
    "        if os.path.exists(vocab_file_path):\n",
    "            dictionary =  build_vocab(None, 0, vocab_dir, vocab_file_path)\n",
    "        else:\n",
    "            data_file_path = download_data_file(download_url, data_dir, data_file_path, expteced_byte)\n",
    "            if lemmatize:\n",
    "                data_file_path = lemmatize_file(data_file_path)\n",
    "            words = read_data(data_file_path)\n",
    "            dictionary = build_vocab(words, vocab_size, vocab_dir, vocab_file_path)\n",
    "            del words\n",
    "\n",
    "        sentence_indices = sentence_to_index(index_file, data_file_path, dictionary)\n",
    "\n",
    "    single_gen = generate_sample(sentence_indices, skip_window)\n",
    "\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1])\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(single_gen)\n",
    "        yield center_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([4199,    0, 5654, 5933, 5933, 4556, 4556, 1501, 1501,    2],\n",
      "      dtype=int32), array([[0.000e+00],\n",
      "       [4.199e+03],\n",
      "       [5.933e+03],\n",
      "       [5.654e+03],\n",
      "       [4.556e+03],\n",
      "       [5.933e+03],\n",
      "       [1.501e+03],\n",
      "       [4.556e+03],\n",
      "       [2.000e+00],\n",
      "       [1.501e+03]]))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "skip_window = 1\n",
    "lemmatize = False\n",
    "gen = batch_gen(download_url, data_dir, data_file_path, expected_byte,\n",
    "               vocab_dir, vocab_file_path, vocab_size, index_file,\n",
    "                batch_size, skip_window, lemmatize)\n",
    "print(next(gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read most common words and write it into a new file to visualize their embeddings on TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_common_words(vocab_file_path, visual_dir, num_visualize):\n",
    "    \"\"\" create a list of num_visualize most frequent words to visualize on TensorBoard.\n",
    "    saved to visual_dir/vocab_[num_visualize].tsv\n",
    "    \"\"\"\n",
    "    words = open(vocab_file_path, 'r').readlines()[:num_visualize]\n",
    "    words = [word for word in words]\n",
    "\n",
    "    safe_mkdir(visual_dir)\n",
    "    file = open(os.path.join(visual_dir, 'vocab_' + str(num_visualize) + '.tsv'), 'w')\n",
    "    for word in words:\n",
    "        file.write(word)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_dir = os.path.join(project_dir, 'visualization')\n",
    "num_visualize = 100\n",
    "most_common_words(vocab_file_path, visual_dir, num_visualize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
