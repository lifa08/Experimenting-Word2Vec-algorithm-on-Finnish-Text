{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voikko\n",
    "\n",
    "Voikko seems to be the only natural language processing tool that can morphologically analyze Finnish words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from voikko.libvoikko import Voikko\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = Voikko(\"fi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Te,WORD>, < ,WHITESPACE>, <olette,WORD>, < ,WHITESPACE>, <esittäneet,WORD>, < ,WHITESPACE>, <toiveen,WORD>, <,,PUNCTUATION>, <                    ,WHITESPACE>, <että,WORD>, < ,WHITESPACE>, <tästä,WORD>, < ,WHITESPACE>, <asiasta,WORD>, < ,WHITESPACE>, <keskusteltaisiin,WORD>, < ,WHITESPACE>, <lähipäivinä,WORD>, < ,WHITESPACE>, <tämän,WORD>, < ,WHITESPACE>, <istuntojakson,WORD>, < ,WHITESPACE>, <aikana,WORD>, <.,PUNCTUATION>]\n",
      "Te\n",
      "1\n",
      "25\n",
      "['Te', 'olette', 'esittäneet', 'toiveen', ',', 'että', 'tästä', 'asiasta', 'keskusteltaisiin', 'lähipäivinä', 'tämän', 'istuntojakson', 'aikana', '.']\n"
     ]
    }
   ],
   "source": [
    "fin_toks = v.tokens(\"Te olette esittäneet toiveen,\\\n",
    "                    että tästä asiasta keskusteltaisiin lähipäivinä tämän istuntojakson aikana.\")\n",
    "# fin_toks is a list of Token objects <tokenText, tokenType>\n",
    "print(fin_toks)\n",
    "print(fin_toks[0].tokenText)\n",
    "print(fin_toks[0].tokenType)\n",
    "print(len(fin_toks))\n",
    "\n",
    "word_toks = [fin_tok.tokenText for fin_tok in fin_toks if fin_tok.tokenType==1 or fin_tok.tokenType==2]\n",
    "print(word_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = Voikko(\"fi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'BASEFORM': 'asia', 'CLASS': 'nimisana', 'FSTOUTPUT': '[Ln][Xp]asia[X]asia[Sela][Ny]sta', 'NUMBER': 'singular', 'SIJAMUOTO': 'sisaeronto', 'STRUCTURE': '=ppppppp', 'WORDBASES': '+asia(asia)'}]\n",
      "asia\n"
     ]
    }
   ],
   "source": [
    "base_form = v.analyze(\"asiasta\")\n",
    "print(base_form)\n",
    "print(base_form[0].get('BASEFORM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['te', ' ', 'olla', ' ', 'esittänyt', ' ', 'toive', ',', ' ', 'että', ' ', 'tämä', ' ', 'asia', ' ', 'keskustella', ' ', 'lähipäivä', ' ', 'tämä', ' ', 'istuntojakso', ' ', 'aika', '.']\n",
      "te olla esittänyt toive, että tämä asia keskustella lähipäivä tämä istuntojakso aika.\n"
     ]
    }
   ],
   "source": [
    "sent = \"Te olette esittäneet toiveen, että tästä asiasta keskusteltaisiin lähipäivinä tämän istuntojakson aikana.\"\n",
    "sent_toks = v.tokens(sent)\n",
    "\n",
    "words_baseform = [v.analyze(word.tokenText)[0].get('BASEFORM') if word.tokenType==1 else word.tokenText\n",
    "                  for word in sent_toks]\n",
    "print(words_baseform)\n",
    "print(''.join(words_baseform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_file(filename):\n",
    "    v = Voikko(\"fi\")\n",
    "    lemmatized_file = open(filename + '_lemmatized', 'w') \n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for sentence in f:\n",
    "            sent_toks = v.tokens(sentence)\n",
    "\n",
    "            words_baseform = []\n",
    "            for word in sent_toks:\n",
    "                if word.tokenType == 1:\n",
    "                    word_analyzed = v.analyze(word.tokenText)\n",
    "                    if len(word_analyzed) > 0:\n",
    "                        words_baseform.append(word_analyzed[0].get('BASEFORM'))\n",
    "                    else:\n",
    "                        words_baseform.append(word.tokenText)\n",
    "                else:\n",
    "                    words_baseform.append(word.tokenText)\n",
    "\n",
    "            sent_baseform = ''.join(words_baseform)\n",
    "            lemmatized_file.write(sent_baseform)\n",
    "\n",
    "    lemmatized_file.close()\n",
    "    v.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/europarl-v8.fi'\n",
    "lemmatize_file(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
